{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Engineering Pipeline Modules.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNg4a3O6HwAlHjvlyQY1qG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riddles1124/wagonden/blob/main/Engineering_Pipeline_Modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHKzSdM4D37X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "IMAGE_SIZE = [200,200,3]\n",
        "con_base = VGG16(weights='imagenet',include_top=False,input_shape=IMAGE_SIZE)\n",
        "con_base.summary()\n",
        "\n",
        "db_r = '/content/drive/Othercomputers/My MacBook Air/Deep Learning/Database/'\n",
        "# os.mkdir(db_home)\n",
        "train_dir = os.path.join(db_r, 'train')\n",
        "# os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(db_r, 'validation')\n",
        "# os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(db_r, 'test')\n",
        "# os.mkdir(test_dir)\n",
        "images = os.path.join(db_r, 'images')\n",
        "# os.mkdir(images)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def local_database_creator(names, file_path_dict):\n",
        "    conn = sqlite3.connect('dog_image_database')\n",
        "    c = conn.cursor()\n",
        "    breeds = [name for name in names]\n",
        "    for breed in breeds:\n",
        "        c.execute(f\"CREATE TABLE IF NOT EXISTS {breed} ([FILEPATH] TEXT)\")\n",
        "    conn.commit()\n",
        "    db = database_creator(file_path_dict)\n",
        "    for ind, bname in enumerate(file_path_dict):\n",
        "        db[ind].to_sql(f'{bname}', conn, if_exists = 'append')\n",
        "    return print(\"Database Created\")\n",
        "\n",
        "breed_folder_names = []\n",
        "for val in names:\n",
        "  breed_folder_names.append(os.path.join(db_home,val))\n",
        "\n",
        "\n",
        "def dict_maker_two(names,breed_folder_names):\n",
        "  dogedict = {}\n",
        "  for ind, name in enumerate(names):\n",
        "    if name == \"Japanese_Spitzes\":\n",
        "      dogedict[name] = [val for val in os.walk(breed_folder_names[ind])]\n",
        "      dogedict[name] = dogedict[name][1][2]\n",
        "    else:\n",
        "      dogedict[name] = [val for val in os.walk(breed_folder_names[ind])]\n",
        "      dogedict[name] = dogedict[name][2][2]\n",
        "  return dogedict\n",
        "\n",
        "def split_dictionary_creator(picdict):\n",
        "  split_dict = {}\n",
        "  for val in picdict.keys():\n",
        "      trainx, testx = tts(picdict[val][1:], test_size = .25, random_state=321)\n",
        "      split_dict[val] = (trainx,testx)\n",
        "  return split_dict  \n",
        "\n",
        "split_dict = split_dictionary_creator(picdict)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(200, 200),\n",
        "    batch_size=100,\n",
        "    class_mode='categorical')\n",
        "\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(200,200),\n",
        "    batch_size=35,\n",
        "    class_mode='categorical')\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(con_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(504, activation='LeakyReLU'))\n",
        "model.add(layers.Dropout(.5))\n",
        "model.add(layers.Dense(252, activation='LeakyReLU'))\n",
        "model.add(layers.Dense(126, activation='softmax'))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "con_base.trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=156,\n",
        "      epochs=50,\n",
        "      validation_data=test_generator,\n",
        "      validation_steps=5,\n",
        "      verbose=1)\n",
        "\n",
        "with open((db_r+'/history.pkl'), 'wb') as h:\n",
        "  pickle.dump(history, h)\n",
        "\n",
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(accuracy))\n",
        "\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4hy4mkleFIVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vpBWuqZ1CkuO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}